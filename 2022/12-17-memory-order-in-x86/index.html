<!doctype html><html lang=zh-hans><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Lindsay"><meta name=description content="A static blog builded by hugo"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet href=/hortensia.min.css><title>浅入 x86 内存序</title></head><body><header><nav><a href=https://error.iorw.io/>[->+&lt;]</a>
<a href=https://error.iorw.io/about>About</a></nav><h1>浅入 x86 内存序</h1><p>2022-12-17 09:10 +0000</p></header><main><pre tabindex=0><code class=language-log data-lang=log>[WARN] Using deprecated method `memory-order&#39;, it might be removed in the future.
</code></pre><h2 id=内存序memory-order>内存序（Memory Order）</h2><p>在 x86 中，内存屏障通常是不需要的，因为 x86 是比较严格的 TSO（不完全，因为始终没有“官宣”，但是按照定义来说比较接近，而且不同的厂商如 AMD/Intel 之间的实现也存在略微差别）。</p><p>百度找到了一份 <a href=https://www.cis.upenn.edu/~devietti/classes/cis601-spring2016/sc_tso.pdf>sc_tso.pdf</a> 讲述了 TSO 的一些特点，其中包括程序顺序（program order）与内存序（memory order）之间的关系：</p><blockquote><ol><li>Whether they are to the same or different addresses (i.e., a=b or a≠b).If L(a) &lt;p L(b) ⇒ L(a) &lt;m L(b) /* Load→Load */If L(a) &lt;p S(b) ⇒ L(a) &lt;m S(b) /* Load→Store */If S(a) &lt;p S(b) ⇒ S(a) &lt;m S(b) /* Store→Store */<del>If S(a) &lt;p L(b) ⇒ S(a) &lt;m L(b)</del> /* Store→Load */: Enable FIFO write buffer</li><li>When to the same address, every load gets its value from the last store before it.
If S(a) &lt;p L(a) ⇒ S(a) &lt;m L(a) /* Store→Load */</li></ol></blockquote><p>其中 <code>&lt;p</code> 表示的是程序顺序下的先后关系，而 <code>&lt;m</code> 则是内存顺序下的先后关系。<strong>注意这里不考虑编译器进行过的指令重排，这里的程序顺序也是编译完后的汇编顺序</strong>。</p><p>内存序其实不一定要完全写回到内存才算，只要其他 CPU 能看到最终更改就行了（globally visible），例如最常见的 Store-Load，假如 Store(a, 100) 执行完后，那么其他任意的核心在执行 Load(a) 时结果都必须保证为 100；但是这不一定意味着内存中的 a 就一定是 100，有可能它还只是在 L3 Cache 中。</p><p>上面的引用处我还是稍微修改了一点，为了方便阅读；可以看到，对于所有的 Load/Store，三种 Load-Load、Load-Store、Store-Store 都是 SC 的，即内存序就是程序序。但是之所以我们说 TSO 要弱于 SC，是因为对于 Store-Load 这样的操作并没有这么严格，Store-Load 只有在当读写同一个地址（同一片内存时）才是成立的；读写不同内存时 Store-Load 下程序序是不能等价于内存序的。</p><p>能作证这一点的还有 <a href=https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html>Intel SDM</a> Volume 3 8.2.2 部分，这里也摘录了一下：</p><blockquote><ol><li>Reads are not reordered with other reads.</li><li>Writes are not reordered with older reads.</li><li>Writes to memory are not reordered with other writes, with the following exceptions:<ul><li>streaming stores (writes) executed with the non-temporal move instructions (MOVNTI, MOVNTQ, MOVNTDQ, MOVNTPS, and MOVNTPD); and</li><li>string operations (see Section 8.2.4.1).</li></ul></li><li>No write to memory may be reordered with an execution of the CLFLUSH instruction; a write may be reordered with an execution of the CLFLUSHOPT instruction that flushes a cache line other than the one being written.1 Executions of the CLFLUSH instruction are not reordered with each other. Executions of CLFLUSHOPT that access different cache lines may be reordered with each other. An execution of CLFLUSHOPT may be reordered with an execution of CLFLUSH that accesses a different cache line.</li><li>Reads may be reordered with older writes to different locations but not with older writes to the same location.</li><li>Reads or writes cannot be reordered with I/O instructions, locked instructions, or serializing instructions.</li><li>Reads cannot pass earlier LFENCE and MFENCE instructions.</li><li>Writes and executions of CLFLUSH and CLFLUSHOPT cannot pass earlier LFENCE, SFENCE, and MFENCE instructions.</li><li>LFENCE instructions cannot pass earlier reads.</li><li>SFENCE instructions cannot pass earlier writes or executions of CLFLUSH and CLFLUSHOPT.</li><li>MFENCE instructions cannot pass earlier reads, writes, or executions of CLFLUSH and CLFLUSHOPT.</li></ol></blockquote><p>其中比较重要的就是上面的第 1、2、3、5 点。翻译下来就是：</p><ul><li>1. Load-Load 下内存序等于程序序（不会被重排）</li><li>2. Load-Store 下内存序等于程序序，注意这里写的是 older reads，也就是说当前 Store 指令（<code>MOV</code> 等）之前的 Load 指令都不会被重新排序</li><li>3. Store-Store 下内存序等于程序序，例外情况是 non-temporal 和 rep mov 之类的指令（后面会提到，在写一般代码时不会太接触到）</li><li>5. Store-Load 下只有当读写的都是同样的地址时内存序才等于程序序，否则对于不同的地址的读写依然会存在重排</li></ul><p>到这里应该大概就了解了为何说 x86 架构普遍都是 TSO，以及为何 TSO 要弱于 SC。</p><h2 id=多核心>多核心</h2><p><strong>注意</strong>上述例子都只在单核心中生效，包括 TSO 这些（写这篇文章的时候好像没考虑到单/多核心问题，可能要回炉重造了 FIXME）。对于多核心系统（multiple-processor），SDM Volume 3 8.2.2 同样也定义了：</p><blockquote><ol><li>Individual processors use the same ordering principles as in a single-processor system.</li><li>Writes by a single processor are observed in the same order by all processors.</li><li>Writes from an individual processor are NOT ordered with respect to the writes from other processors.</li><li>Memory ordering obeys causality (memory ordering respects transitive visibility).</li><li>Any two stores are seen in a consistent order by processors other than those performing the stores</li><li>Locked instructions have a total order.</li></ol></blockquote><p>重点就是，在多核心的情况下，单核心的写入顺序可以保持不变，但是很可能会穿插进其他核心的写操作。SDM 上附带了一个很好的例子来解释这一行为，考虑三个核心都在同时写入数据：</p><pre tabindex=0><code>&gt; Writes from individual processors:
 Processor#1 Processor#2 Processor#3
  Write A.1   Write A.2   Write A.3
  Write B.1   Write B.2   Write B.3
  Write C.1   Write C.2   Write C.3

&gt; Actual writes (one of the example) in memory:
    Write A.1
    Write B.1
    Write A.2
    Write A.3
    Write C.1
    Write B.2
    Write C.2
    Write B.3
    Write C.3
</code></pre><p>P.S. 第四点的“因果律”可能一开始让人觉得有点摸不着头脑，可以参考 <a href=https://stackoverflow.com/a/27374794>Stack Overflow</a> 的例子来了解，实际上就是保证如果单个变量写入且能被正确观测到，那么前面所有的变量写入都应该可以被观测到。个人理解 x86 在多核心的内存序上相对比较“难预测”，内存序依赖于观测到的值，也就是只有在“看”到了实际的值之后，才能知道哪些变量被正确地写入了。</p><h2 id=一个栗子>一个栗子</h2><p>一个我觉得很有“意思”的例子，来源一个知乎文章，介绍<a href=https://zhuanlan.zhihu.com/p/454564295>内存屏障</a>所用的，原文代码有点乱，这里重新格式化并稍微精简了一下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#define _GNU_SOURCE
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;sched.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;pthread.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;assert.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=k>volatile</span> <span class=kt>int</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>r1</span><span class=p>,</span> <span class=n>r2</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>static</span> <span class=n>pthread_barrier_t</span> <span class=n>barrier_start</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>static</span> <span class=n>pthread_barrier_t</span> <span class=n>barrier_end</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>static</span> <span class=kt>void</span><span class=o>*</span> <span class=nf>thread1</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span> <span class=n>a</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>pthread_barrier_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_start</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// run1
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>x</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>r1</span> <span class=o>=</span> <span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>pthread_barrier_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_end</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>static</span> <span class=kt>void</span><span class=o>*</span> <span class=nf>thread2</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span> <span class=n>a</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>pthread_barrier_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_start</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// run2
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>y</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>r2</span> <span class=o>=</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>pthread_barrier_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_end</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>  
</span></span><span class=line><span class=cl>    <span class=n>assert</span><span class=p>(</span><span class=n>pthread_barrier_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_start</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>assert</span><span class=p>(</span><span class=n>pthread_barrier_init</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_end</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>pthread_t</span> <span class=n>t1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>pthread_t</span> <span class=n>t2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>assert</span><span class=p>(</span><span class=n>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=n>t1</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>,</span> <span class=n>thread1</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>assert</span><span class=p>(</span><span class=n>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=n>t2</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>,</span> <span class=n>thread2</span><span class=p>,</span> <span class=nb>NULL</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>cpu_set_t</span> <span class=n>cs</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>CPU_ZERO</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>CPU_SET</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>cs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>assert</span><span class=p>(</span><span class=n>pthread_setaffinity_np</span><span class=p>(</span><span class=n>t1</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>cs</span><span class=p>),</span> <span class=o>&amp;</span><span class=n>cs</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>CPU_ZERO</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>CPU_SET</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>cs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>assert</span><span class=p>(</span><span class=n>pthread_setaffinity_np</span><span class=p>(</span><span class=n>t2</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>cs</span><span class=p>),</span> <span class=o>&amp;</span><span class=n>cs</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// start()
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>x</span> <span class=o>=</span> <span class=n>y</span> <span class=o>=</span> <span class=n>r1</span> <span class=o>=</span> <span class=n>r2</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>pthread_barrier_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_start</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>pthread_barrier_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_end</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// end()
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>assert</span><span class=p>(</span><span class=o>!</span><span class=p>(</span><span class=n>r1</span> <span class=o>==</span> <span class=mi>0</span> <span class=o>&amp;&amp;</span> <span class=n>r2</span> <span class=o>==</span> <span class=mi>0</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>   
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// gcc xxx.c -O2 -pthread &amp;&amp; ./a.out
</span></span></span></code></pre></div><p>根据原作者阐述，这段程序必定会产生 <code>end()</code> 处引发的断言错误。那么我们来对着上面的多核心下的内存模型稍微解释一下吧，根据上面的定义，线程 t1 和线程 t2 同时写入变量 <code>x</code> 与 <code>y</code>，那么真实的写入顺序有可能是先写 <code>x</code> 或是先写 <code>y</code>；不过重点还是“读”的顺序，按照前文关于 Store-Load 序的解释，<code>r1</code> 和 <code>r2</code> <strong>不一定</strong>会读到写入的值（因为不是同一个核心)，x86 内存模型只保证假设真实内存写入顺序为 <code>x = 1; y = 1</code>，那么如果 <code>r1</code> 读到 <code>y == 1</code>，则 <code>r2</code> 必定也能读到 <code>x == 1</code>。</p><p>另外按照 SDM 官方的说法（位于 Volume 3 8.2.3.4），</p><blockquote><p>At each processor, the load and the store are to different locations and hence may be reordered. Any interleaving of the operations is thus allowed.One such interleaving has the two loads occurring before the two stores. This would result in each load returning value 0.</p></blockquote><p>也就是说对于一个核心而言，只有在<strong>该核心</strong>上读写同一地址才是顺序的；否则就算多核心都读写同一地址，中间都有可能发生乱序。这两种解释都是行得通的。</p><p>解决方法可以是下面所述的内存屏障，即在写入 <code>x</code> 或 <code>y</code> 后引入内存屏障，保证下一条读指令执行前完成写入，即：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// t1/run1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>x</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>__asm__</span> <span class=nf>__volatile__</span><span class=p>(</span><span class=s>&#34;mfence&#34;</span> <span class=o>:::</span> <span class=s>&#34;memory&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>r1</span> <span class=o>=</span> <span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// t2/run2
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>y</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>__asm__</span> <span class=nf>__volatile__</span><span class=p>(</span><span class=s>&#34;mfence&#34;</span> <span class=o>:::</span> <span class=s>&#34;memory&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>r2</span> <span class=o>=</span> <span class=n>x</span><span class=p>;</span>
</span></span></code></pre></div><p>这样一来，所有可能的路径都不再会产生 <code>r1</code> 和 <code>r2</code> 同时为零的情况了：</p><ol><li>t1 执行到 <code>x = 1</code>，t2 执行到 <code>mfence</code>，那么 t1 必然能保证 <code>r1 = y = 1</code>，因为 t2 使用了 <code>mfence</code> 强制 CPU 按照 <code>x = 1; r2 = x</code> 的顺序执行（如果没有 <code>mfence</code> 就可能会出现 <code>r2 = x; r1 = y; x = 1; y = 1</code> 的内存读写顺序出现）</li><li>t1 执行到 <code>mfence</code>，t2 执行到 <code>y = 1</code>，同样，t2 必然能保证 <code>r2 = x = 1</code></li></ol><p>注意 <code>x</code> 与 <code>y</code> 是否能够被观测到与指令乱序没有太大关系，内存屏障只保证读写的顺序，但是不保证读写的值是否有效，后者是由 atomic 负责保证的，只是恰巧 x86 下绝大多数读写操作都是原子的，一定要区分好两者的关系 orz。</p><p>另一个粗暴的方法就是所有 mov 操作都带上 lock prefix，保证 total order。</p><p>总而言之，多核心下 x86 内存序能保证的只有写入的顺序，以及“因果律”读到的结果；但是无法保证多核心下读和写的顺序，要保证这种顺序需要依靠内存屏障或是总线锁。</p><h2 id=内存屏障memory-fence>内存屏障（Memory Fence）</h2><p>前文列出的 SDM 卷 3 8.2.2 节里，用了好几个点叙述 <code>xFENCE</code> 指令，包括 <code>LFENCE</code>、<code>SFENCE</code> 和 <code>MFENCE</code>。</p><p>注意到 SDM 卷 3 8.2.5 节里的一句话：</p><blockquote><p>The SFENCE instruction (introduced to the IA-32 architecture in the Pentium III processor) and the LFENCE and MFENCE instructions (introduced in the Pentium 4 processor) provide memory-ordering and serialization capabilities <strong>for specific types of memory operations.</strong></p></blockquote><p>也就是说屏障在 x86 下基本只有“特殊”的内存操作才需要。所以先有一个概念：x86“常规”的指令下不是很需要内存屏障。那么我们继续往下就能看到这三条指令的定义：</p><blockquote><p>The SFENCE, LFENCE, and MFENCE instructions provide a performance-efficient way of ensuring load and store memory ordering between routines that <strong>produce weakly-ordered results</strong> and routines that consume that data. The functions of these instructions are as follows:</p><ul><li>SFENCE — Serializes all store (write) operations that occurred prior to the SFENCE instruction in the program instruction stream, but does not affect load operations.</li><li>LFENCE — Serializes all load (read) operations that occurred prior to the LFENCE instruction in the program instruction stream, but does not affect store operations.2</li><li>MFENCE — Serializes all store and load operations that occurred prior to the MFENCE instruction in the program instruction stream.</li></ul><p><em>2. Specifically, LFENCE does not execute until all prior instructions have completed locally, and no later instruction begins execution until LFENCE completes. As a result, an instruction that loads from memory and that precedes an LFENCE receives data from memory prior to completion of the LFENCE. An LFENCE that follows an instruction that stores to memory might complete before the data being stored have become globally visible. Instructions following an LFENCE may be fetched from memory before the LFENCE, but they will not execute until the LFENCE completes.</em></p></blockquote><p>上面加粗的那句“produce weakly-ordered results”，其实也说明了 x86 不仅仅只有 TSO，为了性能它也有更弱的内存模型（如 ARM、RISC-V 般的）。</p><p>回归正题，稍微地阐述一下这三条内存屏障指令：</p><ul><li><p><code>SFENCE</code> 会保证该指令之前的所有 Store 操作不会“跨过”该指令（可能也是“Serializes”的意思？），但是不对 Load 做任何保证</p></li><li><p><code>LFENCE</code> 则保证该指令之前的所有 Load 操作不会“跨过”该指令，也同样不对 Store 做任何保证；注意这里带了一条尾注，指明 <code>LFENCE</code> 不仅仅能同步数据，也能同步指令，在其他指令没有执行完前不会执行 <code>LFENCE</code>，所以在一个<a href=https://www.zhihu.com/question/29465982/answer/44465936>知乎回答</a>说明了这个特性被用在了内核 <code>RDTSC</code> 之前，以获取更加精确的时钟。</p></li><li><p><code>MFENCE</code> 则会保证之前所有的 Store/Load 操作都不会“跨过”该指令；注意在汇编的世界中（我认为）单条指令永远不等价于多条指令的“和”，例如这里的 <code>MFENCE != SFENCE + LFENCE</code>，原因就是“原子性”，如果用 <code>SFENCE + LFENCE</code> 替代 <code>MFENCE</code> 的话，有个比较大的问题就是两条指令中间有可能存在一定的“空窗期”：在两条指令中间，虽然 <code>SFENCE</code> 保证了当前核心之前的所有 Store 指令都完成了，但是现在空窗期间（执行 <code>LFENCE</code> 前）我们已经不能再保证 Store-Store 序了，因为其他 CPU 可能会执行任何的读写操作。这一点在 <a href=https://preshing.com/20120710/memory-barriers-are-like-source-control-operations/#storeload>Preshing 的文章</a>中也提及到了：</p><blockquote><p>However, those two barrier types are insufficient. Remember, the push operation may be delayed for an arbitrary number of instructions, and the pull operation might not pull from the head revision.</p></blockquote><p>所以在汇编上我们必须要有一个 single instrument 来保证 atomicity，这跟 <code>inc</code>、<code>xchg</code> 等“混合”指令的作用是类似的，虽然它们都能拆分成多个指令，但是为了保证原子性必须要提供（就算非常精简以及 weakly-ordered 的 RISC-V 架构依然存在着 <code>xchg</code> 这样的指令，来保证原子性）。</p></li></ul><p>总而言之，通常操作下我们并不需要关心 x86 的内存屏障，因为它已经“足够的强”了，但是面对其他情况下依然需要屏障。</p><p>P.S. 比较有意思的一段话：</p><blockquote><p>Intel does not guarantee that future processors will support this model.</p></blockquote><p>看来 Intel 也非常想废弃掉 TSO 啊（笑）。</p><h2 id=sequentially-consistent>Sequentially Consistent</h2><p>内存屏障的其中一个作用，就是将 TSO 内存序变为更强的 SC 内存序，具体做法参考 <a href=https://stackoverflow.com/a/27635527>StackOverflow 的答案</a>，里面列出了四种做法：</p><blockquote><ol><li><code>LOAD</code> (without fence) and <code>STORE</code> + <code>MFENCE</code></li><li><code>LOAD</code> (without fence) and <code>LOCK XCHG</code></li><li><code>MFENCE</code> + <code>LOAD</code> and <code>STORE</code> (without fence)</li><li><code>LOCK XADD</code> ( 0 ) and <code>STORE</code> (without fence)</li></ol></blockquote><p>并且指出 GCC 使用的就是方法 1，也就是 <code>STORE + MFENCE</code> 就能将 TSO 转变为 SC。这个比较好理解，因为 <code>MFENCE</code> 保证了指令之前的<strong>所有读写</strong>操作都对其他核心可见，那么如果我们每次写的时候都执行一次 <code>MFENCE</code>，就意味着每次写完就去强制读写可见，那么后面的读就必定只会读到最新的指。另外因为 SC 要保证 StoreLoad，所以就必须要 <code>MFENCE</code>（？）。</p><h2 id=non-temporal>Non-Temporal</h2><p>上面提到了对于 NT（Non-Temporal）操作，我们是不能保证 Store-Store 序的，也就是说一个 NT 指令后立刻接上另外的 Store 指令，那么最终的结果是无法确定的，这也是 x86 中 relaxed-order 的体现。所以对于所有的 NT 指令，都建议带上 <code>SFENCE</code> 内存屏障。</p><blockquote><p>Non-Temporal SSE instructions (MOVNTI, MOVNTQ, etc.), don&rsquo;t follow the normal cache-coherency rules. Therefore non-temporal stores must be followed by an SFENCE instruction in order for their results to be seen by other processors in a timely fashion.</p><p>When data is produced and not (immediately) consumed again, the fact that memory store operations read a full cache line first and then modify the cached data is detrimental to performance. This operation pushes data out of the caches which might be needed again in favor of data which will not be used soon. This is especially true for large data structures, like matrices, which are filled and then used later. Before the last element of the matrix is filled the sheer size evicts the first elements, making caching of the writes ineffective.</p><p>For this and similar situations, processors provide support for non-temporal write operations. Non-temporal in this context means the data will not be reused soon, so there is no reason to cache it. These non-temporal write operations do not read a cache line and then modify it; instead, the new content is directly written to memory.</p><p>Source: <a href=http://lwn.net/Articles/255364/>http://lwn.net/Articles/255364/</a> <a href=https://stackoverflow.com/a/37092>https://stackoverflow.com/a/37092</a></p></blockquote><p>NT 与否其实是跟 cache 有很大关系，这里的解释说是 Non-Temporal 的所有操作都不走 cache（所以才叫“非临时”？）。还是翻一下 SDM 吧，首先直接找到 <code>MOVNTDQA</code> 的指令介绍吧，在卷 2 4.3 中，摘录关于 Non-Temporal 的部分：</p><blockquote><p>The non-temporal hint is implemented by using a write combining (WC) memory type protocol when reading the data from memory. Using this protocol, the processor does not read the data into the cache hierarchy, nor does it fetch the corresponding cache line from memory into the cache hierarchy. The memory type of the region being read can override the non-temporal hint, if the memory address specified for the non-temporal read is not a WC memory region. Information on non-temporal reads and writes can be found in “Caching of Temporal vs. Non-Temporal Data” in Chapter 10 in the Intel® 64 and IA-32 Architecture Software Developer’s Manual, Volume 3A.</p><p>Because the WC protocol uses a weakly-ordered memory consistency model, a fencing operation implemented with a MFENCE instruction should be used in conjunction with MOVNTDQA instructions if multiple processors might use different memory types for the referenced memory locations or to synchronize reads of a processor with writes by other agents in the system. A processor’s implementation of the streaming load hint does not override the effective memory type, but the implementation of the hint is processor dependent. For example, a processor implementation may choose to ignore the hint and process the instruction as a normal MOVDQA for any memory type. Alternatively, another implementation may optimize cache reads generated by MOVNTDQA on WB memory type to reduce cache evictions.</p></blockquote><p>这里简要地说明了这个指令的一些细节，例如它的作用就是将内存中的数据读到寄存器中，但是不经过缓存层（也不触发任何的 fetch），第二段则指出了该指令工作在 weakly-ordered 的内存序下，因此需要借助前文说的内存屏障来保证可见性（以及顺序性）。顺便这里也指路到了 SDM 卷 1 10.4.6.2（跟原文的卷号对不上，标错了？）：</p><blockquote><p>Data referenced by a program can be temporal (data will be used again) or non-temporal (data will be referenced once and not reused in the immediate future). For example, program code is generally temporal, whereas, multimedia data, such as the display list in a 3-D graphics application, is often non-temporal. <strong>To make efficient use of the processor’s caches, it is generally desirable to cache temporal data and not cache non-temporal data.</strong> Overloading the processor’s caches with non-temporal data is sometimes referred to as “polluting the caches.” The SSE and SSE2 cacheability control instructions enable a program to write non-temporal data to memory in a manner that minimizes pollution of caches.</p></blockquote><p>也就是说 NT 宽泛来说（概念性）指的就是不会立刻被使用到的数据，也因此出于性能考虑也就不再进行缓存，将缓存让给 temporal data。因此从实现角度来说 NT 就是不经过缓存的数据，考据终了。</p><p>P.S. 在 SDM 卷 3 11.3 中能找到所有的 Memory Types 和相关解释。不贴原文了，整理（粗略地理解）了一下：</p><ul><li>Write Combining (WC)：对于一些无法被缓存（cache）的项目，会存在一个专门的 buffer（write combining buffer）来收集并“聚合”（combine）这些读写操作并推迟写入，以此来减少内存（或者 I/O）的读写次数；一些 serializing 指令（如上面提到的 <code>SFENCE</code>、<code>MFENCE</code> 以及 <code>CPUID</code> 等）会强行落地并清空 WC Cache。更多关于 WC Buffer 可以参考 <a href=https://stackoverflow.com/a/49961612>StackOverflow 上的答案</a>，比较复杂，我也没仔细地去看了 orz。</li><li>Write-through (WT)：所有的 Load/Store 都走 cache，但是每次在往 cache 中写入（Store）时都会直接同步到内存中；读取（Load）则依然会从 cache 中读取。因此 WT 比较适合写不敏感，但是要求与内存保持同步的一些场景（I/O？）。</li><li>Write-back (WB)：所有的 Load/Store 都走 cache，数据会保持在 cache 中直到缓存策略决定该何时写回到内存。这就是我们最常用（也是性能最好的）一种 cache 模式。但是对于 I/O 等需要访问内存的外设（device）肯定就需要考虑一致性的问题了，同时也需要考虑缓存一致性的问题（cache coherency）。</li><li>Write protected (WP)：只对 Load 操作走 cache，所有的 Store 的操作都会让缓存失效。</li></ul><p>P.P.S. 有一个 <code>PREFETCH</code> 指令可以显式地将部分内存载入到缓存中，这也是在内核代码里看到很多 prefetch 的原理。具体可以看指令说明。</p><h2 id=follymemcpys>folly/memcpy.S</h2><p>这里我的兴趣点在于 <code>.L_NON_TEMPORAL_LOOP</code> 这个 label 上，首先可以找到哪里会跳转到这里来：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=err>//</span> <span class=nf>This</span> <span class=nv>threshold</span> <span class=nv>is</span> <span class=nv>half</span> <span class=nv>of</span> <span class=nv>L1</span> <span class=nv>cache</span> <span class=nv>on</span> <span class=nv>a</span> <span class=nv>Skylake</span> <span class=nv>machine</span><span class=p>,</span> <span class=nv>which</span> <span class=nv>means</span> <span class=nv>that</span>
</span></span><span class=line><span class=cl><span class=err>//</span> <span class=nf>potentially</span> <span class=nb>al</span><span class=nv>l</span> <span class=nv>of</span> <span class=nv>L1</span> <span class=nv>will</span> <span class=nv>be</span> <span class=nv>populated</span> <span class=nv>by</span> <span class=nv>this</span> <span class=nv>copy</span> <span class=nv>once</span> <span class=nv>it</span> <span class=nv>is</span> <span class=nv>executed</span>
</span></span><span class=line><span class=cl><span class=err>//</span> <span class=err>(</span><span class=nf>dst</span> <span class=nv>and</span> <span class=nv>src</span> <span class=nv>are</span> <span class=nv>cached</span> <span class=nv>for</span> <span class=nv>temporal</span> <span class=nv>copies</span><span class=p>)</span><span class=nv>.</span>
</span></span><span class=line><span class=cl><span class=err>#</span><span class=nf>define</span> <span class=nv>NON_TEMPORAL_STORE_THRESHOLD</span> <span class=kc>$</span><span class=mi>32768</span>
</span></span><span class=line><span class=cl><span class=nf>...</span>
</span></span><span class=line><span class=cl><span class=nf>cmp</span>         <span class=nv>NON_TEMPORAL_STORE_THRESHOLD</span><span class=p>,</span> <span class=o>%</span><span class=nb>rdx</span>
</span></span><span class=line><span class=cl><span class=nf>jae</span>         <span class=nv>.L_NON_TEMPORAL_LOOP</span>
</span></span></code></pre></div><p>也就是说当我们 <code>memcpy</code> 的大小超过了 L1 缓存（folly 取了 Skylake 作为基准）后，就不再借助缓存去拷贝了，而是直接利用上述的 <code>MOVNTx</code> 系列的 NT 指令绕过缓存，实现比较高性能的拷贝：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-nasm data-lang=nasm><span class=line><span class=cl><span class=nl>.L_NON_TEMPORAL_LOOP:</span>
</span></span><span class=line><span class=cl>        <span class=nf>testb</span>       <span class=kc>$</span><span class=mi>31</span><span class=p>,</span> <span class=o>%</span><span class=nb>si</span><span class=nv>l</span>
</span></span><span class=line><span class=cl>        <span class=nf>jne</span>         <span class=nv>.L_ALIGNED_DST_LOOP</span>
</span></span><span class=line><span class=cl>        <span class=err>//</span> <span class=nf>This</span> <span class=nv>is</span> <span class=nv>prefetching</span> <span class=nv>the</span> <span class=nv>source</span> <span class=nv>data</span> <span class=nv>unlike</span> <span class=nb>AL</span><span class=nv>IGNED_DST_LOOP</span> <span class=nv>which</span>
</span></span><span class=line><span class=cl>        <span class=err>//</span> <span class=nf>prefetches</span> <span class=nv>the</span> <span class=nv>destination</span> <span class=nv>data.</span> <span class=nv>This</span> <span class=nb>ch</span><span class=nv>oice</span> <span class=nv>is</span> <span class=nv>again</span> <span class=nv>informed</span> <span class=nv>by</span>
</span></span><span class=line><span class=cl>        <span class=err>//</span> <span class=nf>benchmarks.</span> <span class=nv>With</span> <span class=nv>a</span> <span class=nv>non</span><span class=o>-</span><span class=nv>temporal</span> <span class=nv>store</span> <span class=nv>the</span> <span class=nv>entirety</span> <span class=nv>of</span> <span class=nv>the</span> <span class=nv>cache</span> <span class=nv>line</span>
</span></span><span class=line><span class=cl>        <span class=err>//</span> <span class=nf>is</span> <span class=nv>being</span> <span class=nv>written</span> <span class=nv>so</span> <span class=nv>the</span> <span class=nv>previous</span> <span class=nv>data</span> <span class=nv>can</span> <span class=nv>be</span> <span class=nb>di</span><span class=nv>scarded</span> <span class=nv>without</span> <span class=nv>being</span>
</span></span><span class=line><span class=cl>        <span class=err>//</span> <span class=nf>fetched.</span>
</span></span><span class=line><span class=cl>        <span class=nf>prefetchnta</span> <span class=mi>128</span><span class=p>(</span><span class=o>%</span><span class=nb>rsi</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>prefetchnta</span> <span class=mi>196</span><span class=p>(</span><span class=o>%</span><span class=nb>rsi</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdqa</span>   <span class=p>(</span><span class=o>%</span><span class=nb>rsi</span><span class=p>),</span> <span class=o>%</span><span class=nv>ymm0</span>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdqa</span>   <span class=mi>32</span><span class=p>(</span><span class=o>%</span><span class=nb>rsi</span><span class=p>),</span> <span class=o>%</span><span class=nv>ymm1</span>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdqa</span>   <span class=mi>64</span><span class=p>(</span><span class=o>%</span><span class=nb>rsi</span><span class=p>),</span> <span class=o>%</span><span class=nv>ymm2</span>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdqa</span>   <span class=mi>96</span><span class=p>(</span><span class=o>%</span><span class=nb>rsi</span><span class=p>),</span> <span class=o>%</span><span class=nv>ymm3</span>
</span></span><span class=line><span class=cl>        <span class=nf>add</span>         <span class=kc>$</span><span class=mi>128</span><span class=p>,</span> <span class=o>%</span><span class=nb>rsi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdq</span>    <span class=o>%</span><span class=nv>ymm0</span><span class=p>,</span> <span class=p>(</span><span class=o>%</span><span class=nb>rdi</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdq</span>    <span class=o>%</span><span class=nv>ymm1</span><span class=p>,</span> <span class=mi>32</span><span class=p>(</span><span class=o>%</span><span class=nb>rdi</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdq</span>    <span class=o>%</span><span class=nv>ymm2</span><span class=p>,</span> <span class=mi>64</span><span class=p>(</span><span class=o>%</span><span class=nb>rdi</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>vmovntdq</span>    <span class=o>%</span><span class=nv>ymm3</span><span class=p>,</span> <span class=mi>96</span><span class=p>(</span><span class=o>%</span><span class=nb>rdi</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nf>add</span>         <span class=kc>$</span><span class=mi>128</span><span class=p>,</span> <span class=o>%</span><span class=nb>rdi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nf>cmp</span>         <span class=o>%</span><span class=nv>r8</span><span class=p>,</span> <span class=o>%</span><span class=nb>rsi</span>
</span></span><span class=line><span class=cl>        <span class=nf>jb</span>          <span class=nv>.L_NON_TEMPORAL_LOOP</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nf>sfence</span>
</span></span><span class=line><span class=cl>        <span class=nf>jmp</span>         <span class=nv>.L_ALIGNED_DST_LOOP_END</span>
</span></span></code></pre></div><p>可以看到 folly 借助 <code>movntdqa</code> 指令将数据读入到 AVX 寄存器 <code>ymmX</code> 中，再使用 <code>movntdq</code> 将寄存器的值写回到内存，并循环。最后，注意到这里还有个 <code>sfence</code> 指令，保证在拷贝完后数据对其他核心可见（但是拷贝途中无法保证，所以可能需要调用着确保不会出现 data race），符合 SDM 上的定义。</p><p>到此，基本上我们也理解了 x86 的 TSO 内存序（比 SC 弱的地方在于只能保证同一个地址的 Store-Load，但是可以通过 <code>MFENCE</code> 指令做到所有地址的 Store-Load 以实现 SC）、x86 的内存屏障的作用（通常程序中因为 TSO 的存在没有什么作用，但是对于如 NT 系列的特殊指令需要 weakly-ordered memory，就必须使用内存屏障了；不过为了程序的 portable 性，还是尽量地加上内存屏障操作，也能加深理解）。现在，我们再来尝试理解（困扰我多年的.jpg）C++ <code>std::memory_order</code> 了。</p><h2 id=cache>Cache</h2><p>关于 Memory Order，偶尔还是会提到缓存，因为像 Intel x86 等主流实现来说，基本都会带有缓存。那么我们对内存进行操作的时候就会存在一个中间的 cache 状态，这可能会导致内存序出现一些小小的差异（例如如果有一个简单的 cache，我们往同一个内存地址多次写入数据，那么实际就只是一直在往 cache 中写入数据，没有<em>及时地</em>写回到内存中去，其他 CPU 就无法观测，也就无法保证 Store-Load 等内存序了）。</p><p>这个问题就交由缓存一致性算法来解决，最终实现的效果就是在 x86 下无论我们怎么操作 cache 和内存，只要地址相同，就能保证其他 CPU 始终能观测到最新值，从而保证 Store-Load 序正确（上面的例子也就不会出现无法观测的问题）。而对于其他架构而言，应该也需要实现类似的效果，起码是需要有处理 atomic 的能力，即能让别的 CPU 观测到对内存操作的变化。</p><p>对于 x86 而言，常用的缓存一致性协议是 MESI，这个协议比较重要的就是“嗅探机制”（snoop），也是通过 snoop 通过总线来得知其他 CPU 的操作；具体可以查看<a href=https://en.wikipedia.org/wiki/MESI_protocol>维基百科 MESI</a> 的页面，这部分我暂时还没深入地去了解，大概只知道这些了（逃。</p><h2 id=编译器的指令重排>编译器的指令重排</h2><p>有时候在大量存在变量的读写时，编译器会选择性地进行指令重排（例如你能在 GDB 中发现很多“诡异”的跳转，常常执行到一半的时候跳回到开头执行变量的初始化等），这就是区别于 CPU 指令重排的编译器指令重排。编译器重排基本都是在编译期做的，有两种方式可以抑制：</p><ol><li>变量使用 <code>volatile</code> 关键字，针对的是该变量；</li><li>使用 <code>asm volatile("" ::: "memory")</code>，针对的是该语句上下的重排，跟 CPU 的内存屏障很像，就是避免语句之前的读写操作重新排序到该语句之后。另外语句起作用的地方是 <code>"memory"</code>，所以第一个参数无论是任何汇编指令都会有抑制编译器重排的效果。</li></ol><p>大概也就是这些了，文章重点依然在 x86 上，暂时不讨论编译器了（其实是不会 QAQ）。</p><p>后续会写一篇 C/C++ 的 Memory Order，来对“真实世界”中的内存模型（希望能）有一个更稍微透彻的理解<del>以及咕咕咕</del>。</p></main><footer>&copy;2023 &#183;
<a href=https://error.iorw.io>Byte</a> &#183;
<a href=https://github.com/hartlottery target=_blank rel=noopener>M-x</a></footer></body></html>